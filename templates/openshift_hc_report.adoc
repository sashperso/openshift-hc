:pdf-theme: ./styles/pdf/redhat-theme.yml
:pdf-fontsdir: ./fonts
:subject: Consulting Engagement Report
:docstatus: {{ docstatus }}
:icons: font
:doctype: book
:revnumber: {versiondate}
:subject: Red Hat OpenShift Health Check Report  
:toc:

= Red Hat OpenShift Health Check Report

:sectnums:
== *Disclaimer*

_This health check provides a best effort evaluation of the targeted OCP cluster(s). The results from the
report do not in any way provide a guarantee or warranty for the health and performance of the targeted
OCP cluster(s). The health check aims to provide an indication of overall health and provide consultant
recommendations on a best effort basis._


This health check has been conducted by {{ AUTHORNAME }} for {{ CUSTOMERNAME }}.

== *Introduction*
Red Hat is a leading global provider of open-source software solutions, renowned for its commitment to providing enterprise-level solutions built on the principles of open collaboration. Founded in 1993, the company has played a pivotal role in advancing the adoption of Linux and open-source technologies in corporate environments. One of Red Hat's flagship products is OpenShift, a robust container orchestration platform that simplifies the deployment, scaling, and management of containerized applications. OpenShift leverages Kubernetes, an open-source container orchestration system, and extends its capabilities to offer a comprehensive container platform with features such as automated scaling, load balancing, and integrated security. This platform facilitates the development and operation of cloud-native applications, allowing organizations to embrace the agility and scalability of containerization while maintaining the stability and security required for enterprise environments. Red Hat's dedication to open-source values and its innovative solutions, such as OpenShift, have positioned it as a key player in the ever-evolving landscape of modern IT infrastructure.


Conducting an OpenShift Health Check is a strategic move for organizations leveraging this container orchestration platform, offering a range of benefits crucial for maintaining optimal performance and security. Firstly, a health check ensures that the OpenShift environment is aligned with recommended practices, identifying potential misconfigurations or vulnerabilities that could compromise system integrity. This proactive approach helps preemptively address issues before they escalate, minimizing downtime and enhancing overall system reliability. Additionally, a health check provides insights into resource utilization and performance bottlenecks, allowing for fine-tuning and optimization of the OpenShift deployment to maximize efficiency. Security concerns are addressed through a comprehensive review, identifying and remedying potential vulnerabilities. Conducting the health check in a container provides the health check with the required dependencies smoothly and with complete isolation from the host system. Ultimately, an OpenShift Health Check contributes to the long-term stability of the platform, ensuring that it continues to meet the evolving needs of the organization while maintaining a secure and well-performing containerized infrastructure.


Harnessing the efficiency of automation with Ansible and Containerization technology, our Health Check process seamlessly operates within a container, delivering a user-friendly and insightful PDF report. This streamlined workflow involves providing authentication credentials to the OpenShift Container Platform (OCP) cluster, constructing a container image equipped with all necessary dependencies for the Health Check, establishing a designated directory for the container to mount and generate the PDF, and finally, executing the container with the custom-built image. The resulting PDF can be easily customized to meet the specific needs of our clients by our dedicated consultants.

== *Key*
The OCP Healthcheck uses the following key to indicate the status of the cluster. 
****
image::{{ pass_icon }} 
Good: The cluster follows recommended practices and indicates the status is satisfactory.

image::{{ warning_icon }}
Warning: The cluster does not follow recommended practices, which may become areas of concern in the future, which is up to the individual's discretion. 

image::{{ fail_icon }}
Bad: The cluster definitely does not follow recommended practices and poses an immediate risk. 
****

== *Executive Summary*
{% if EXECUTIVESUMMARY == '' %}
image::{{ warning_icon }}
Executive Summary has not been added, please add it in the parameter 'EXECUTIVESUMMARY' in the settings/config file. 
{% else %}
{{ EXECUTIVESUMMARY }}
{% endif %}

== *Cluster Overview*
The details about the OpenShift cluster and command line (CLI) versions are listed below.  
 
----
OpenShift `oc` CLI Client Version: *{{ cluster_cli_version }}*
OpenShift Cluster (Server) Version: *{{ cluster_version }}*
----

=== Cluster Information
Summary of key cluster information. 

----
Details:
Cluster API address: {{ cluster_api }}
Cluster ID: {{ cluster_id }}
OpenShift Cluster Version: {{ cluster_version }}
Update channel: {{ cluster_update_channel }}
Update Available: {{ cluster_update }}
OpenShift Installation method: {{ cluster_installation_check }}
OpenShift DNS Name: {{cluster_dns_name }}
Openshift Network Type: {{ cluster_network_type }} 

Cluster Stats:
Number of namespaces: {{ cluster_namespaces }}
Number of builds: {{ cluster_builds }}
Number of secrets: {{ cluster_secrets }}
Number of config maps: {{ cluster_configmaps }}
Number of services: {{ cluster_services }}
Number of build configs: {{ cluster_bc }}
Number of custom resource definitions (CRD): {{ cluster_crds }}
----

{% if ClUSTER_RECOMMENDATION|length > 0 %}
**Consultant Recommendations**


`{{ ClUSTER_RECOMMENDATION }}`
{% endif %}

=== Nodes Information
This section shows information of all the nodes present in the cluster, and lists their status, role, Operating System and their versions and when the nodes were created. 


==== Nodes 
----
{{ nodes }} 
----
==== Nodes that are in 'Not Ready' state
This health check looks into the statuses of each nodes and lists if there are any non-working nodes. 

****
{% if nodes_not_ready|length == 0 %}
image::{{ pass_icon }}
All nodes are working successfully. 
{% else %}
image::{{ fail_icon }}
All nodes are not ready. Please review.
----
{{ nodes_not_ready }}
---- 
{% endif %}
****
{% if NODE_RECOMMENDATION|length > 0 %}
**Consultant Recommendations**


`{{ NODE_RECOMMENDATION }}`
{% endif %}

=== Pods Information
Pods are critical to how OpenShift runs its operations and applications. This section checks into their status and returns observations of non-working pods. 

==== Pods Not Running
The number of pods not running shows which pods in the entire OCP cluster are not running. If there are none, the logical implementation of the check returns a message reflecting the status. This check looks into the number of pods across the cluster that are not running successfully. 

****
{% if "Result: All pods are in Running state, no errors as of now" in pods_not_running %}
image::{{ pass_icon }}
{{ pods_not_running | to_nice_yaml }}
{% else %}
image::{{ warning_icon }}
{{ pods_not_running | to_nice_yaml }}
{% endif %}
****

==== Pods Restarted
Pods that have containers which have restarted for more than the `restart threshold` (as set in the settings/config file) is pointed out and observed in this health check, and the the logical implementation also returns a no error message if there no pods with that threshold amount of container restarts. 
****

{% set unstable_pods = [] %}
{% set unknown_pods = [] %}

{% for pod in ocp_pods %}
{%   set container_restarts = 0 %}
{%   if pod.status.containerStatuses is defined %}
{%     for cs in pod.status.containerStatuses %}
{%       set container_restarts = container_restarts + cs.restartCount %}
{%     endfor %}
{%     if container_restarts > RESTART_THRESHOLD %}
{%       set unstable_pods = unstable_pods.append([pod,container_restarts]) %}
{%     endif %}
{%   else %}
{%     set unknown_pods = unknown_pods.append(pod) %}
{%   endif %}
{% endfor %}

{% if unstable_pods|length > 0 %}
image::{{ fail_icon }}
The following pods have restarted more times than the threshold limit of {{RESTART_THRESHOLD }}:

[%header, cols="2,2,1"]
|====
| Pod Name 
| Namespace
| Container Restarts
{%   for upod in unstable_pods %}
| {{ upod[0].metadata.name }}
| {{ upod[0].metadata.namespace }}
| {{ upod[1] }}

{%   endfor %}
|====
{% else %}
image::{{ pass_icon }}
All pods fall within the restart threshold of {{ RESTART_THRESHOLD }} container restarts.
{% endif %}

{% if unknown_pods|length > 0 %}
The following pods have an unknown Container Status, indicating that they are pending or experiencing errors with its containers. Please investigate.

[%header, cols="2,2"]
|====
| Pod Name 
| Namespace

{% for pod in unknown_pods %}
| {{ pod.metadata.name }}
| {{ pod.metadata.namespace }}

{% endfor %}
|====

{% endif %}

****

{% if PODS_RECOMMENDATION|length > 0 %}
**Consultant Recommendations**


`{{ PODS_RECOMMENDATION }}`
{% endif %}

== *Certificate Signing Request*
****
{% if csr_pending == '0' %}
image::{{ pass_icon }}
There are  {{ csr_pending }} pending Certificate Signing Requests (CSRs) in the cluster. 
{% else %}
image::{{ warning_icon }}
There are  {{ csr_pending }} pending Certificate Signing Requests (CSRs) in the cluster. Please review if they need to be signed.
{% endif %}
****

{% if csr_pending != "0" %}
These should be reviewed as soon as possible- unapproved CSRs can stop the nodes from becoming ready if they have have been recently added, or if the cluster has restarted.
{% endif %}

{% if CSR_RECOMMENDATION|length > 0 %}
**Consultant Recommendations**


`{{ CSR_RECOMMENDATION }}`
{% endif %}

== *MachineConfig Information*

The following check gets the names of machine config pools and other relevant information. 
----
{{ machine_config_pools_name }}
----

The following breaks down which nodes are associated into which machine config pool.
----
{{ nodes_mcp }}
----
Degraded machine counts refer to the number of machines in your OCP cluster that are experiencing issues or are in a degraded state. This would affect application availability and resource utilisation (Preferred State is zero). 
****
{% if 'No counts of degraded mcps' in degraded_mcps %}
image::{{ pass_icon }}
Degraded MCPs Status:
{{ degraded_mcps }}
{% else %}
image::{{ fail_icon }}
Degraded MCPs Status:
{{ degraded_mcps }}
{% endif %}
****
Nodes may be in a pending state that may eventuate to the degraded state. The preferred state is each Machine Config Pool having a 0 value. (Read the following as first mcp's unavailable value is the first character of the string and so on)

****
{% for i in my_list %}
{% if '0' in i %}
image::{{ pass_icon }}
----
{{ i }}
----
{% else %}
image::{{ fail_icon }}
----
{{ i }}
----
{% endif %}
{% endfor %}
****


{% if MACHINECONFIG_RECOMMENDATION|length > 0 %}
**Consultant Recommendations**


`{{ MACHINECONFIG_RECOMMENDATION }}`
{% endif %}

[NOTE]
====
For recommended practice guidelines, please use the below links. +
1. https://access.redhat.com/solutions/5244121 +
2. https://docs.openshift.com/container-platform/4.10/rest_api/machine_apis/machineconfigpool-machineconfiguration-openshift-io-v1.html
====

== *Resource Quotas*
The check displays the hard and used limits. This helps with resource allocation, and review if the used limit is not approaching the hard limit. 
----
Name: 
{{ resource_quota_name }}

Hard Limit:
{{ resource_quota_hard_limit }}

Used Limit:
{{ resource_quota_used_limit }}
----

{% if RESOURCE_RECOMMENDATION|length > 0 %}
**Consultant Recommendations**


`{{ RESOURCE_RECOMMENDATION }}`
{% endif %}
[NOTE]
====
For recommended practice guidelines, please use the below links. +
1. https://docs.openshift.com/container-platform/4.8/applications/quotas/quotas-setting-per-project.html
====

== *Image Registry*

The Management State of the Image Registry Operator alters the behaviour of the deployed image pruner job. 

* 'Managed' means the --prune-registry flag for image pruner is set to true (preferred state).
* 'Removed' means the --prune-registry flag for the image pruner is set to false, meaning it only prunes image metadata in etcd.
* 'Unmanaged' means the --prune-registry flag for the image pruner is set to false. 



****
{% if management_state_registry  == 'Managed' %}
image::{{ pass_icon }}
{% elif management_state_registry == 'Removed' %}
image::{{ warning_icon }}
{% elif management_state_registry == 'Unmanaged' %}
image::{{ fail_icon }}
{% endif %}
Management State: {{ management_state_registry }}
****


Builder images are base images that contain the necessary tools and runtime for building and compiling source code into executable applications. Builder images are used as a foundation for creating application  images. They are often provided by Openshift, the community, or can be custom-built to suit specific development environments and languages. 

This check is assuming the images are in the openshift-image-registry namespace and/or master nodes. 

The check has found the following images that are not provided by releases of Red Hat and OpenShift. Please review the health of these images through Red Hat Advanced Cluster Security and/or through organisational policies. 
****
{% if external_images_node  == '' %}
image::{{ pass_icon }}
{% else %}
image::{{ warning_icon }}
{% endif %}
External images on node: {{ external_images_node }}
****

****
{% if external_images_registry_namespace  == '' %}
image::{{ pass_icon }}
{% else %}
image::{{ warning_icon }}
{% endif %}
External images in namespace: 

{{ external_images_registry_namespace }}
****

{% if IMAGEREGISRTY_RECOMMENDATION|length > 0 %}
**Consultant Recommendations**


`{{ IMAGEREGISRTY_RECOMMENDATION }}`
{% endif %}

[NOTE]
====
For recommended practice guidelines, please use the below links. +
1. https://access.redhat.com/documentation/en-us/openshift_container_platform/4.8/html-single/registry/index +
2. https://all.docs.genesys.com/PrivateEdition/Current/PEGuide/OCR +
3. https://docs.openshift.com/container-platform/4.8/registry/configuring-registry-operator.html
====

== *Operators*
=== List of Cluster Service Versions that have not Succeeded

****
{% if CSV_STATUS|length == 0 %}
image::{{ pass_icon }}
There are no CSV's in unsuccessful state and are all healthy. 
{% else %}
image::{{ fail_icon }}
Please check the following Cluster Service Versions that are in unsuccessful state and may not be healthy. 
{{ CSV_STATUS }}
{% endif %}
****

=== Incomplete Install Plans 

{% set incomplete_installplans = [] %}
{% for ip in install_plans %}
{%   if ip.status.phase != "Complete" %}
{%     set incomplete_installplans = incomplete_installplans.append(ip) %}
{%   endif %}
{% endfor %}

****
{% if incomplete_installplans|length == 0 %}
image::{{ pass_icon }}
All the Install Plans for the Operators subscriptions have been approved. 
{% else %}
image::{{ warning_icon }}
The following Install Plans have been found to be incomplete. Please review and approve accordingly to ensure all operator installations are completed and accounted for.

[%header, cols="2,2,2,2"]
|====
| Operator Resources 
| Install Plan Name
| Namespace
| Phase

{%   for ip in incomplete_installplans %}
a|

{%     for csv in ip.spec.clusterServiceVersionNames %}
* {{csv}}
{%     endfor %}

| {{ ip.metadata.name }}
| {{ ip.metadata.namespace }}
| {{ ip.status.phase }}

{%   endfor %}
|====
{% endif %}
****

=== List of operators 
----
{{ OPERATORS }}
----

{% if OPERATORS_RECOMMENDATION|length > 0 %}
**Consultant Recommendations**


`{{ OPERATORS_RECOMMENDATION }}`
{% endif %}

== *Networks*
The network check looks into the entire OCP cluster and observes which Ingress policies have not been admitted to a network. 


=== List of routes that have not been admitted
****
{% if routes_not_admitted|length == 0 %}
image::{{ pass_icon }}
There are no routes that have not been admitted, which reflects that all the Ingress policies in the cluster have been admitted. 
{% else %}
image::{{ warning_icon }}
Please check the following routes that have not been admitted, and please act accordingly to oranisational policies. 
{{ routes_not_admitted }}
{% endif %}
****

{% if NETWORKS_RECOMMENDATION|length > 0 %}
**Consultant Recommendations**


`{{ NETWORKS_RECOMMENDATION }}`
{% endif %}

== *Persistent Storage*

Persistent storage in OpenShift uses the Kubernetes persistent volume (PV) framework that allows cluster administrators to provision persistent storage for a cluster. Developers use persistent volume claims (PVCs) to request PV resources without having specific knowledge of the underlying storage infrastructure.  PVCs are specific to a project while PV resources on their own are not scoped to any single project. After a PV is bound to a PVC, that PV can not then be bound to additional PVCs.  PVCs can exist in the system that are not owned by any container. This may be intentional, if the PVC is to be retained for future use.

=== Storage Classes

StorageClass objects describes and classifies storage that can be requested and serve as a management mechanism for controlling different levels of storage and access to that storage.  

The following storage classes are defined in the cluster:

[%header, cols="2,2"]
|===
| Name
| Provisioner

{% set sc_default = "" %}
{% set sc_defaultcount = 0 %}

{% for sc in storage_classes %}
| {{ sc.metadata.name }}{% if sc.annotations['storageclass.kubernetes.io/is-default-class'] is defined %} **\(default\)**{% endif %}
| {{ sc.provisioner }}

{%   if sc.annotations['storageclass.kubernetes.io/is-default-class'] is defined %}
{%     set sc_defaultcount = sc_defaultcount + 1 %}
{%   endif %}
{% endfor %}
|===

{% if sc_defaultcount == 0 %}
[WARNING]
====
No Default Class has been set. Please refer to the following documentation and resolve to avoid unintended indefinite pending of future storage allocations:

- https://docs.openshift.com/container-platform/4.14/storage/container_storage_interface/persistent-storage-csi-sc-manage.html#absent-default-storage-class

====
{% elif sc_defaultcount > 1 %}
[WARNING]
====
Multiple default classes have been set. Please refer to the following documentation and reduce the amount of defaults to 1 to avoid unintended Storage allocation behaviours in the cluster:

- https://docs.openshift.com/container-platform/4.14/storage/container_storage_interface/persistent-storage-csi-sc-manage.html#multiple-default-storage-classes

====
{% endif %}

{% if STORAGECLASS_RECOMMENDATION|length > 0 %}
**Consultant Recommendations**


`{{ STORAGECLASS_RECOMMENDATION }}`
{% endif %}

=== Bound PersistentVolumeClaims

The following list of PersistentVolumeClaims (PVC) are defined and bound to an underlying Persistent Volume (PV) in the cluster across all namespaces:

{% for pvc in bound_pvcs %}

[cols="1,1"]
|===
|**Name **
|{{ pvc.name }}

|**Namespace**
|{{ pvc.namespace }}

|**Storage Class**
|{{ pvc.storageclass }}

|**Capacity**
|{{ pvc.capacity }}

|**Access Modes**
|{{ pvc.accessmodes }}

|===

{% endfor %}

{% if unbound_pvcs|length > 0 %}
=== Un-Bound PVCs

The following list of PersistentVolumeClaims (PVC) are defined and are not bound to any underlying Persistent Volume (PV) in the cluster across all namespaces:

{% for pvc in unbound_pvcs %}

[cols="1,1"]
|===
|**Name **
|{{ pvc.name }}

|**Namespace**
|{{ pvc.namespace }}

|**Storage Class**
|{{ pvc.storageclass }}

|**Capacity**
|{{ pvc.capacity }}

|**Access Modes**
|{{ pvc.accessmodes }}

|===

{% endfor %}

PVCs may be unbound for multiple reasons.  For example: some storage classes will only bind a PVC to a PV when it is actually used.  This may be a desired state.  In other cases, a PVC may not be able to bind to a PV if the Storage Class can not satisfy the storage request if, for example, there is insufficient space available, or if the PVC is requesting an access mode not supported by that Storage Class.

**Consultant Recommendations**

`{{ UNBOUND_PV_RECOMMENDATION }}`
{% endif %}

{% if orphaned_pvcs|length > 0 %}
=== Unowned PVCs

PVCs can exist in the system that are not owned by any container. This may be intentional if, for example, the PVC has been released by an application but it is intended to be reused by another application. Alternatively, the PVC may need to be manually deleted.

Results: 

{% for pvc in orphaned_pvcs %}
- {{ pvc }}

{% endfor %}

**Consultant Recommendations**

`{{ ORPHANED_PV_RECOMMENDATION }}`
{% endif %}

{% if unowned_pvs|length > 0 %}
=== Unowned PVs

Persistent Volumes (PV) can exist in the system that are not owned by any PVC. This may be intentional if, for example, the PV delete policy is set to __Retain__ so that they can be manually deleted after the PVC is deleted.

Results: 

{% for pv in unowned_pvs %}
- {{ pv.name }}

{% endfor %}

**Consultant Recommendations**

`{{ UNOWNED_PV_RECOMMENDATION }}`
{% endif %}

== *Kubelets*
The following checks are occurring on the master nodes. 

=== Authentication
Anonymous authentication should be preferably set to false, in order for users to identify themselves before authentication to API.

{% for i in anonymous_authentication %}
{% if 'The node is' in i %}
****
{{ i }}

{% elif 'false' in i %}

image::{{ pass_icon }}

{{ i }}
****
{% else %}

image::{{ fail_icon }}

{{ i }}
****
{% endif %}
{% endfor %}

=== Pods
podsPerCore sets the number of pods the node can run based on the number of processor cores on the node. podsPerCore cannot exceed maxPods (default state of maxPods is 250 pods with 4096 podPidsLimit).
****
{% for i in kubelet_pods %}
{{ i }}

{% endfor %}
****
=== APIs
The rate at which the kubelet talks to the API server depends on queries per second (QPS) and burst values. The default values 50 for kubeAPIQPS and 100 for kubeAPIBurst, are good enough if there are limited pods running on each node. Updating the kubelet QPS and burst rates is recommended if there are enough CPU and memory resources on the node.
****
{% for i in kubelet_APIs %}
{{ i }}

{% endfor %}
****
=== Rotate Certificates
Having rotateCertificates enabled makes sure that nodes are more consistently available, whilst certificates may expire.

{% for i in kubelet_rotate_certificate %}
{% if 'The node is' in i %}
****
{{ i }}

{% elif 'false' in i %}

image::{{ fail_icon }}

{{ i }}
****
{% else %}

image::{{ pass_icon }}

{{ i }}
****
{% endif %}
{% endfor %}

=== CgroupDriver
Cgroupfs and systemd are the predominant cgroup drivers. The preferred driver is systemd as it is tightly integrated with cgroups and will assign a cgroup to each systemd unit. Using cgroupfs with systemd means that there will be two different cgroup managers( ie two views of the resources)

{% for i in kubelet_cgroupDriver %}
{% if 'The node is' in i %}
****
{{ i }}

{% elif 'cgroupfs' in i %}

image::{{ warning_icon }}

{{ i }}
****
{% else %}

image::{{ pass_icon }}

{{ i }}
****
{% endif %}
{% endfor %}

=== CgroupRoot
CgroupRoot should be the root directory. Ensuring that the kubelet service file ownership is set to root.

{% for i in kubelet_cgroupRoot %}
{% if 'The node is' in i %}
****
{{ i }}

{% elif '"/"' in i %}

image::{{ pass_icon }}

{{ i }}
****
{% else %}

image::{{ fail_icon }}

{{ i }}
****
{% endif %}
{% endfor %}

=== Permissions
Ensuring that the kubelet service file permissions are set to 644 or more restrictive.



{% for i in actual_kubelet_permissions.stdout_lines %}
{% if 'The node is' in i %}
****
{{ i }}
{% elif i|int <= 644 %}

image::{{ pass_icon }}

The permission on the kubelet service file is restrictive with {{ i }}
****
{% else %}
****
image::{{ fail_icon }}

The permission on the kubelet service file is not restrictive with {{ i }}. Please review permissions
****
{% endif %}
{% endfor %}



----
{{ kubelet_permission }}
----
==== clusterDNS
The IP address Pods are using for DNS resolution.
----
{{ kubelet_clusterDNS }}
----

{% if KUBELETS_RECOMMENDATION|length > 0 %}
**Consultant Recommendations**


`{{ KUBELETS_RECOMMENDATION }}`
{% endif %}

[NOTE]
====
For recommended practice guidelines, please use the below links. +
1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ +
2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/
====

== *Alerts*

=== Alert rules

This table shows which alerts have been 'Active' and 'Fired'. The Alerts are a great indication, defined by rules using Prometheus Query Language (PQL) of what is potentially going wrong with the cluster. 

[cols="2,2,1,2"]
|===
| Name | NameSpace | Severity | ActiveSince

{% for i in range(alerts_firing_names.stdout_lines | length) %}

| {{ alerts_firing_names.stdout_lines[i]   }} | {{ alerts_firing_namespace.stdout_lines[i]   }} | {{ alerts_firing_severity.stdout_lines[i]  }} | {{ alerts_firing_active_at.stdout_lines[i] }}


{% endfor %}
|===


=== Grafana
****
{%if grafana_enabled.stdout|length > 0 %}
image::{{ pass_icon }}
Grafana is enabled in this cluster. 
{% else %}
image::{{ warning_icon }}
Grafana is not present in the cluster. It may have been deprecated. Please check release notes.
{% endif %}
****
=== Prometheus
This checks the prometheus pods running the cluster are running successfully or not. The ContainerReady section looks into the number of ready containers against the total number of containers in the pod. 
[cols="1,1,1"]
|===
| Name | ContainerReady | Status

{% for i in range(prom_pods_name.stdout_lines | length) %}

| {{ prom_pods_name.stdout_lines[i]   }} | {{ prom_pods_container_ready.stdout_lines[i]   }} | {{ prom_pods_status.stdout_lines[i]  }} 

{% endfor %}
|===

{% if ALERTS_RECOMMENDATION|length > 0 %}
**Consultant Recommendations**


`{{ ALERTS_RECOMMENDATION }}`
{% endif %}

{#
== *Etcd*
The etcd pods that are running. 
----
{{ etcd_pods }}
----
Fast disks are the most critical factor for etcd deployment performance and stability. A slow disk will increase ETCD request latency and potentially hurt cluster stability. Because etcd maintains a detailed record of its keyspace over time, it's necessary to regularly condense this history to prevent performance issues and avoid running out of storage space. Compacting the keyspace history removes information about keys that are no longer relevant before a specific revision, making the space used by these keys available for new data. The compaction process should be quick, ideally below 100ms (typically below 10ms for fast storage types like SSD/NVMe or AWS io1) for smaller clusters, but it can take up to 800ms for larger clusters (20 or more workers). Anything beyond 800ms could lead to performance problems.



This Health check is checking compaction rate and is assuming its for a large cluster and rounding off to closest integer of milliseconds.
****
{% if (etcd_time | split(' ') | last | split ('ms') | first | int) < 800 %}
image::{{ pass_icon }}
Compaction Rate:   {{ etcd_time | split(' ') | last | split ('ms') | first | int }} milliseconds
{% elif 800 < (etcd_time | split(' ') | last | split ('ms') | first | int) < 900 %}
image::{{ warning_icon }}
Compaction Rate:  {{ etcd_time | split(' ') | last | split ('ms') | first | int }} milliseconds
{% else %}
image::{{ fail_icon }}
Compaction Rate:  {{ etcd_time | split(' ') | last | split ('ms') | first | int }} milliseconds
{% endif %}
****

The following is conducting the fio test (by spinning up a container in the master node, some crazy calculations happening in background, just retrieving the last important lines)  and checks the results provide the 99th percentile of fsync and if it is in the recommended threshold to host etcd or not. 
****
{{ fio_results1 }}


{% if 'the disk can be used to host etcd' in fio_results2 %}
image::{{ pass_icon }}
{{ fio_results2 }}
{% else %}
image::{{ fail_icon }}
{{ fio_results2 }}
{% endif %}
****

Please review following comprehensive table for health of etcd endpoints, compaction rate for each endpoint and any further error messages regarding etcd. 

[%autowidth]
----
{{ etcd_table }}
----

{% if ETCD_RECOMMENDATION|length > 0 %}
**Consultant Recommendations**


`{{ ETCD_RECOMMENDATION }}`
{% endif %}

[NOTE]
==== 
For recommended practice guidelines, please use the below links. +
1. https://docs.openshift.com/container-platform/4.13/scalability_and_performance/recommended-performance-scale-practices/recommended-etcd-practices.html +
2. https://access.redhat.com/solutions/4885641
====
#}

== Cluster Resource Allocation
This section covers the accumulative resource request allocations across the nodes of the target cluster at the time this health check was ran. Resource limits can be altered using the **Cluster Override Operator** in order to allow over-commiting of node resources.

Listed below are the nodes and their current resource allocations

{% for node in cluster_override_nodes %}
**{{ node }}**

[%header, cols="1,1,1"]
|===
| Resource
| Current Request
| Over Limit

{% for array in range(cluster_override_array | int) %}
| {{ override_set_statistics[node][array].resource }}{set:cellbgcolor}
| {{ override_set_statistics[node][array].current_request }}%
| {% if override_set_statistics[node][array].overlimit == 'fine' %}{{ override_set_statistics[node][array].overlimit }}{% else %}{{ override_set_statistics[node][array].overlimit }}%{% endif %}

{% endfor %}
|===

{% endfor %}

<<<

== *Certificates*

This section summarises the amount of certificates currently defined in the cluster, as well as highlight how many have expired. A detailed table of certificates is provided in the Appendix.

{% set ocp_tls_certificates_expired = {} %}
{% for i in ocp_tls_certificates %}
{% if i.expired == True %}
{% set ocp_tls_certificates_expired = ocp_tls_certificates_expired + [i] %}
{% endif %}
{% endfor %}

*Number of Certificates in cluster:* {{ ocp_tls_certificates | length }} +
*Number of Expired Certificates:* {{ ocp_tls_certificates_expired | length }}

**Consultant Recommendations**

{% if CERTIFICATES_RECOMMENDATION|length > 0 %}
**Consultant Recommendations**
`{{ CERTIFICATES_RECOMMENDATION }}`
{% endif %}

<<<

== *Pod Disruption Budgets*

Pod Disruption Budgets ensure that a desired minimun number of service instances are running in OpenShift at any given time. Any PDBs found to be in violation will indicate that there are potential service outages within the cluster, potentially causing instability.

{% set pdb_badlist = [] %}
{% for pdb in poddisruptionbudgets %}
  {% if pdb.status.currentHealthy < pdb.status.desiredHealthy %}
    {% set pdb_badlist = pdb_badlist.append(pdb) %}
  {% endif %}
{% endfor %}

{% if pdb_badlist|length > 0 %}
The following *{{ pdb_badlist|length }}* unhealthy PDBs were found at the time of this report:

[%header, cols="2,2"]
|====
| Name 
| Desired Healthy
| Current Healthy
  {% for pdb in pdb_badlist %}
| {{ pdb.metadata.name }}
| {{ pdb.status.desiredHealthy }}
| {{ pdb.status.currentHealthy }}

  {% endfor %}
|====
{% else %}
No unhealthy PDBs found at the time of this report.
{% endif %}

[NOTE]
====
For recommended practice guidelines, please refer to the following concerning pod preemption: +
https://docs.openshift.com/container-platform/4.13/nodes/pods/nodes-pods-priority.html#nodes-pods-priority-preempt-about_nodes-pods-priority
====

:sectnums!:

<<<

== Appendix

=== *Certificates*

[%header, cols="2,2"]
|====
| CN 
| Status

{% for i in ocp_tls_certificates %}
{% if i.subject.commonName is defined %}
| {{ i.subject.commonName }}
{% else %}
| {{ i.subject }}
{% endif %}
| {{ i.expired }}

{% endfor %}
|====