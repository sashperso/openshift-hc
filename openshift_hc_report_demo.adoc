:pdf-theme: ./styles/pdf/redhat-theme.yml
:pdf-fontsdir: ./fonts
:subject: Consulting Engagement Report
:docstatus: draft

:toc:

= Red Hat OpenShift Health Check Report

:sectnums:
== *Disclaimer*

_This health check provides a best effort evaluation of the targeted OCP cluster(s). The results from the
report do not in any way provide a guarantee or warranty for the health and performance of the targeted
OCP cluster(s). The health check aims to provide an indication of overall health and provide consultant
recommendations on a best effort basis._


This health check has been conducted by Harry-Potter for Voldemort.


== *Cluster Overview*
Details of the OpenShift cluster are listed below. This information reflects the cluster version and type. 


The OC cli version of the bash: 
----
OpenShift `oc` CLI Client Version: * 4.13.8*
OpenShift Cluster (Server) Version: *4.14.1*
----

=== Cluster Information:
----
Details:
Cluster API address: https://api.cluster-rz44p.dynamic.opentlc.com:6443
Cluster ID: ca1b647a-8143-4f5c-b3d0-077167fea54c
OpenShift Cluster Version: 4.14.1
Update channel: stable-4.14
Update Available: 4.14.2
OpenShift Installation method: 
OpenShift DNS Name: cluster-rz44p.dynamic.opentlc.com
Openshift Network Type: OVNKubernetes 

Cluster Stats:
Number of namespaces: 71
Number of builds: 0
Number of secrets: 1047
Number of config maps: 598
Number of services: 100
Number of build configs: 0
Number of custom resource definitions (CRD): 153
----


=== Nodes Information:
This section shows information of all the nodes present in the cluster, and lists their status, role, Operating System and their versions and when the nodes were created. 


==== Nodes: 
----
 
----
==== Nodes that are in 'Not Ready' state:
This health check looks into the statuses of each nodes and lists if there are any non-working nodes. A blank section reflects all the nodes are in a working successfully. 

----

---- 

**Consultant Recommendations**


`Nodes are in good health.`

=== Pods Information:
Pods are critical to how OpenShift runs its operations and applications. This section checks into their status and returns observations of non-working pods. 

==== Pods Not Running
The number of pods not running shows which pods in the entire OCP cluster is not running. If there are none, the logical implementation of the check returns a message reflecting the status. 

----
- This check looks into the number of pods across the cluster that are not running
    successfully.
- 'Result: All pods are in Running state, no errors as of now.'

----
==== Pods Restarted
Pods that have containers which have restarted for more than a `threshold` is pointed out and observed in this health check, and the the logical implementation also returns a no error message if there no pods with that threshold amount of container restarts. 

----
- This check looks into the number of pods that have restarted after for 6 times or
    more.
- 'Result: None of the pods have restarted.'

----

**Consultant Recommendations**


`Quite a few containers in pods have restarted. This maybe due to ..`

== *CSR*
----
- ['pending csrs', '0']
----

== *MachineConfig Information*:

The following check gets the names of machine config pools and other relevant information for a consultant's discretion. 
----
NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master   rendered-master-f0e7a40f6f33086028c242a5ccf2c540   True      False      False      3              3                   3                     0                      6h15m
worker   rendered-worker-5c4df110e2e0412fb8452a4319cf8918   True      False      False      3              3                   3                     0                      6h15m
----
The following breaks down which nodes are associated into which machine config pool.
----
NAME                            STATUS   ROLES                  AGE     VERSION
control-plane-cluster-rz44p-1   Ready    control-plane,master   6h4m    v1.27.6+f67aeb3
control-plane-cluster-rz44p-2   Ready    control-plane,master   6h21m   v1.27.6+f67aeb3
control-plane-cluster-rz44p-3   Ready    control-plane,master   6h20m   v1.27.6+f67aeb3
worker-cluster-rz44p-1          Ready    worker                 6h5m    v1.27.6+f67aeb3
worker-cluster-rz44p-2          Ready    worker                 6h4m    v1.27.6+f67aeb3
worker-cluster-rz44p-3          Ready    worker                 6h5m    v1.27.6+f67aeb3
----
Degraded machine counts refer to the number of machines in your OCP cluster that are experiencing issues or are in a degraded state. This would affect application availability and resource utilisation. (Preferred State is zero)
****
Degraded MCPs Status:

[.green]#This check looks into the degraded MachineConfigPools.
Result: No counts of degraded mcps.#
****
Nodes may be in a pending state that may eventuate to the degraded state, whilst the OCP Health Check is occurring. The preferred state is each Machine Config Pool having a 0 value. (Read the following as first mcp's unavailable value is the first character of the string and so on)

****
Unavailablenodes:
[.green]#['machineconfigpool.machineconfiguration.openshift.io/master', '0']#
[.green]#['machineconfigpool.machineconfiguration.openshift.io/worker', '0']#
****
****
Check performed on following nodes: [['machineconfigpool.machineconfiguration.openshift.io/master', '0'], ['machineconfigpool.machineconfiguration.openshift.io/worker', '0']]
****


== *Resource Quotas*:
----
Name: 
['host-network-namespace-quotas']

Hard Limit:
['{"count/daemonsets.apps":"0","count/deployments.apps":"0","limits.cpu":"0","limits.memory":"0","pods":"0"}']

Used Limit:
['{"count/daemonsets.apps":"0","count/deployments.apps":"0","limits.cpu":"0","limits.memory":"0","pods":"0"}']
----


== *Image Registry*

The Management State of the Image Registry Operator alters the behaviour of the deployed image pruner job. 

* 'Managed' means the --prune-registry flag for image pruner is set to true (preferred state).
* 'Removed' means the --prune-registry flag for the image pruner is set to false, meaning it only prunes image metadata in etcd.
* 'Unmanaged' means the --prune-registry flag for the image pruner is set to false. 



****
Management State: [.green]#Managed#
****


Builder images are base images that contain the necessary tools and runtime for building and compiling source code into executable applications. Builder images are used as a foundation for creating application  images. They are often provided by Openshift, the community, or can be custom-built to suit specific development environments and languages. 

This check is assuming the images are in the openshift-image-registry namespace and/or master nodes. 

The check has found the following images that is not provided by releases of Red Hat and OpenShift. Please review the health of these images through Red Hat Advanced Cluster Security and/or through organisational policies. 
****
External images on node: [.orange]#REPOSITORY                                  TAG         IMAGE ID      CREATED       SIZE#
****
****
External images in namespace: [.green]#No external images found on namespaces#
****


== *Operators*
=== List of Cluster Service Versions that have not Succeeded:
An empty section reflects that there are no CSV's in unsuccessful state and are all healthy. 

----

----
=== List of install plans that have not been approved: 
An empty section reflects that all the Install Plans for the Operators subscriptions have been approved. 

----

----
=== List of operators: 
----
NAME                                        AGE
mcg-operator.openshift-storage              5h45m
ocs-operator.openshift-storage              5h45m
odf-csi-addons-operator.openshift-storage   5h45m
odf-operator.openshift-storage              5h45m
----


== *Networks*
The network-check looks into the entire OCP cluster and observes which Ingress policies have not been admitted to a network. 


=== List of routes that have not been admitted:
An empty section reflects that all the Ingress policies in the cluster have been admitted. 

----

----


== *Persistent Storage*

Persistent storage in OpenShift uses the Kubernetes persistent volume (PV) framework that allows cluster administrators to provision persistent storage for a cluster. Developers use persistent volume claims (PVCs) to request PV resources without having specific knowledge of the underlying storage infrastructure.  PVCs are specific to a project while PV resources on their own are not scoped to any single project. After a PV is bound to a PVC, that PV can not then be bound to additional PVCs.  PVCs can exist in the system that are not owned by any container. This may be intentional, if the PVC is to be retained for future use.

=== Storage Classes

StorageClass objects describes and classifies storage that can be requested and serve as a management mechanism for controlling different levels of storage and access to that storage.  

The following storage classes are defined in the cluster:

[cols="1,1"]
|===
|**Name**
|managed-nfs-storage

|**Provisioner**
|rhpd/nfs

|**Default**
| false

|===
[cols="1,1"]
|===
|**Name**
|ocs-external-storagecluster-ceph-rbd

|**Provisioner**
|openshift-storage.rbd.csi.ceph.com

|**Default**
| true

|===
[cols="1,1"]
|===
|**Name**
|openshift-storage.noobaa.io

|**Provisioner**
|openshift-storage.noobaa.io/obc

|**Default**
| 

|===


=== Bound PersistentVolumeClaims

The following list of PersistentVolumeClaims (PVC) are defined and bound to an underlying Persistent Volume (PV) in the cluster across all namespaces:


[cols="1,1"]
|===
|**Name **
|pvc-image-registry

|**Namespace**
|openshift-image-registry

|**Storage Class**
|ocs-external-storagecluster-ceph-rbd

|**Capacity**
|20Gi

|**Access Modes**
|['ReadWriteOnce']

|===


[cols="1,1"]
|===
|**Name **
|db-noobaa-db-pg-0

|**Namespace**
|openshift-storage

|**Storage Class**
|ocs-external-storagecluster-ceph-rbd

|**Capacity**
|50Gi

|**Access Modes**
|['ReadWriteOnce']

|===


[cols="1,1"]
|===
|**Name **
|noobaa-default-backing-store-noobaa-pvc-14243819

|**Namespace**
|openshift-storage

|**Storage Class**
|ocs-external-storagecluster-ceph-rbd

|**Capacity**
|50Gi

|**Access Modes**
|['ReadWriteOnce']

|===





== *Kubelets*:
The following checks are occurring on the master nodes. 

=== Authentication
Anonymous authentication should be preferably set to false, in order for users to identify themselves before authentication to API.
****
[.green]#The node is control-plane-cluster-rz44p-1
      "enabled": false
The node is control-plane-cluster-rz44p-2
      "enabled": false
The node is control-plane-cluster-rz44p-3
      "enabled": false#
****
=== Pods
podsPerCore sets the number of pods the node can run based on the number of processor cores on the node.podsPerCore cannot exceed maxPods (default state of maxPods is 250 pods with 4096 podPidsLimit)
****
The node is control-plane-cluster-rz44p-1
  "maxPods": 250,
  "podPidsLimit": 4096,
The node is control-plane-cluster-rz44p-2
  "maxPods": 250,
  "podPidsLimit": 4096,
The node is control-plane-cluster-rz44p-3
  "maxPods": 250,
  "podPidsLimit": 4096,
****
=== APIs
The rate at which the kubelet talks to the API server depends on queries per second (QPS) and burst values. The default values 50 for kubeAPIQPS and 100 for kubeAPIBurst, are good enough if there are limited pods running on each node. Updating the kubelet QPS and burst rates is recommended if there are enough CPU and memory resources on the node.
****
The node is control-plane-cluster-rz44p-1
  "kubeAPIQPS": 50,
  "kubeAPIBurst": 100,
The node is control-plane-cluster-rz44p-2
  "kubeAPIQPS": 50,
  "kubeAPIBurst": 100,
The node is control-plane-cluster-rz44p-3
  "kubeAPIQPS": 50,
  "kubeAPIBurst": 100,
****
=== Rotate Certificates
Having rotate certificates enabled makes sure that nodes are more consistently available, whilst certificates may expire.
****
[.green]#The node is control-plane-cluster-rz44p-1
  "rotateCertificates": true,
The node is control-plane-cluster-rz44p-2
  "rotateCertificates": true,
The node is control-plane-cluster-rz44p-3
  "rotateCertificates": true,#
****
=== CgroupDriver
Cgroupfs and systemd are the predominant cgroup drivers. The preferred driver is systemd as it is tightly integrated with cgroups and will assign a cgroup to each systemd unit. Using cgroupfs with systemd means that there will be two different cgroup managers( ie two views of the resources)
****
[.green]#The node is control-plane-cluster-rz44p-1
  "cgroupDriver": "systemd",
The node is control-plane-cluster-rz44p-2
  "cgroupDriver": "systemd",
The node is control-plane-cluster-rz44p-3
  "cgroupDriver": "systemd",#
****
=== CgroupRoot
CgroupRoot should be roots directory. Ensuring that the kubelet service file ownership is set to root.
****
[.green]#The node is control-plane-cluster-rz44p-1
  "cgroupRoot": "/",
The node is control-plane-cluster-rz44p-2
  "cgroupRoot": "/",
The node is control-plane-cluster-rz44p-3
  "cgroupRoot": "/",#
****
=== Permissions
Ensuring that the kubelet service file permissions are set to 644 or more restrictive.
----
The node is control-plane-cluster-rz44p-1
-rw-r--r--. 1 root root 2906 Nov 22 23:29 /etc/kubernetes/kubelet.conf
The node is control-plane-cluster-rz44p-2
-rw-r--r--. 1 root root 2906 Nov 22 23:29 /etc/kubernetes/kubelet.conf
The node is control-plane-cluster-rz44p-3
-rw-r--r--. 1 root root 2906 Nov 22 23:29 /etc/kubernetes/kubelet.conf
The node is worker-cluster-rz44p-1
-rw-r--r--. 1 root root 2906 Nov 22 23:29 /etc/kubernetes/kubelet.conf
The node is worker-cluster-rz44p-2
-rw-r--r--. 1 root root 2906 Nov 22 23:29 /etc/kubernetes/kubelet.conf
The node is worker-cluster-rz44p-3
-rw-r--r--. 1 root root 2906 Nov 22 23:29 /etc/kubernetes/kubelet.conf
----
==== clusterDNS
The IP address Pods are using for DNS resolution.
----
  "clusterDNS": [
    "172.31.0.10"
----


== *Alerts*

=== Alert rules:

This table shows which alerts have been 'Active' and 'Fired'. The Alerts are a great indication, defined by rules using Prometheus Query Language (PQL) of what is potentially going wrong with the cluster. 

[cols="2,2,1,2" , options="unbreakable"]
|====
| Name | NameSpace | Severity | ActiveSince


| ClusterNotUpgradeable | openshift-cluster-version | info | 2023-11-22T23:29:54.354531345Z



| UpdateAvailable | openshift-cluster-version | info | 2023-11-22T23:14:43.732059958Z



| SimpleContentAccessNotAvailable | openshift-insights | info | 2023-11-22T23:14:36.842179642Z



| Watchdog | openshift-monitoring | none | 2023-11-22T23:14:25.73046059Z



| TargetDown | openshift-storage | warning | 2023-11-22T23:30:25.900289054Z



| AlertmanagerReceiversNotConfigured | openshift-monitoring | warning | 2023-11-22T23:14:45.572611761Z


|====


=== Grafana
[.orange]#Grafana is not present in the cluster. It may have been deprecated. Please check release notes.#

=== Prometheus
This checks the prometheus pods running the cluster are running successfully or not. The ContainerReady section looks into the number of ready containers against the total number of containers in the pod. 
[cols="2,2,2" , options="unbreakable"]
|====
| Name | ContainerReady | Status

|====


== *Etcd*:
The etcd pods that are running
----
etcd-control-plane-cluster-rz44p-1
etcd-control-plane-cluster-rz44p-2
etcd-control-plane-cluster-rz44p-3
etcd-guard-control-plane-cluster-rz44p-1
etcd-guard-control-plane-cluster-rz44p-2
etcd-guard-control-plane-cluster-rz44p-3
----
Fast disks are the most critical factor for etcd deployment performance and stability. A slow disk will increase ETCD request latency and potentially hurt cluster stability. Because etcd maintains a detailed record of its keyspace over time, it's necessary to regularly condense this history to prevent performance issues and avoid running out of storage space. Compacting the keyspace history removes information about keys that are no longer relevant before a specific revision, making the space used by these keys available for new data. The compaction process should be quick, ideally below 100ms (typically below 10ms for fast storage types like SSD/NVMe or AWS io1) for smaller clusters, but it can take up to 800ms for larger clusters (20 or more workers). Anything beyond 800ms could lead to performance problems.



This Health check is checking compaction rate and is assuming its for a large cluster and rounding off to closest integer of milliseconds.
****
Compaction Rate:   [.green]#59 milliseconds#
****

The following is conducting the fio test (by spinning up a container in the master node, some crazy calculations happening in background, just retrieving the last important lines)  and checks the results provide the 99th percentile of fsync and if it is in the recommended threshold to host etcd or not. 
****
INFO: 99th percentile of fsync is 4489216 ns


[.green]#INFO: 99th percentile of the fsync is within the recommended threshold: - 10 ms, the disk can be used to host etcd#
****

Please review following comprehensive table for health of etcd endpoints, compaction rate for each endpoint and any further error messages regarding etcd. 

[%autowidth]
----
+--------------------------+--------+-------------+-------+
|         ENDPOINT         | HEALTH |    TOOK     | ERROR |
+--------------------------+--------+-------------+-------+
| https://10.10.10.11:2379 |   true | 11.064725ms |       |
| https://10.10.10.10:2379 |   true | 11.500212ms |       |
| https://10.10.10.12:2379 |   true | 14.408095ms |       |
+--------------------------+--------+-------------+-------+

----


:sectnums!:

== *References*:

Etcd:
[%autowidth]
----
https://docs.openshift.com/container-platform/4.13/scalability_and_performance/recommended-performance-scale-practices/recommended-etcd-practices.html
https://access.redhat.com/solutions/4885641
----

MachineConfigPools
[%autowidth]
----
1. https://access.redhat.com/solutions/5244121
2. https://docs.openshift.com/container-platform/4.10/rest_api/machine_apis/machineconfigpool-machineconfiguration-openshift-io-v1.html
----

ResourceQuotas
[%autowidth]
----
1. https://docs.openshift.com/container-platform/4.8/applications/quotas/quotas-setting-per-project.html
----

Kubelets 
[%autowidth]
----
1. https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/
2. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/
----

ImageRegistry
[%autowidth]
----
1. https://access.redhat.com/documentation/en-us/openshift_container_platform/4.8/html-single/registry/index
2. https://all.docs.genesys.com/PrivateEdition/Current/PEGuide/OCR
3. https://docs.openshift.com/container-platform/4.8/registry/configuring-registry-operator.html
----