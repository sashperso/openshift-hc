:pdf-theme: ./styles/pdf/redhat-theme.yml
:pdf-fontsdir: ./fonts
include::vars/document-vars.adoc[]

= Red Hat OpenShift Health Check Report

=== Disclaimer

This health check provides a best effort evaluation of the targeted OCP cluster(s). The results from the
report do not in any way provide a guarantee or warranty for the health and performance of the targeted
OCP cluster(s). The health check aims to provide an indication of overall health and provide consultant
recommendations on a best effort basis.

The OC cli version of the bash: 
----
{{ cluster_cli_version | to_nice_yaml }}
----


This is a health check report for Red Hat OpenShift cluster version {{ cluster_version }}.

----
- 'Client Version: 4.13.8'
- 'Kustomize Version: v4.5.7'
- 'Server Version: 4.12.12'
- 'Kubernetes Version: v1.25.7+eab9cc9'
----

== Cluster Overview

=== Cluster Information:
----
Details:
Cluster API address: {{ cluster_api }}
Cluster ID: {{ cluster_id }}
OpenShift Cluster Version: {{ cluster_version }}
Update channel: {{ cluster_update_channel }}
Update Available: {{ cluster_update }}
OpenShift Installation method: {{ cluster_instalation_check }}
OpenShift DNS Name: {{cluster_dns_name }}
Openshift Network Type: {{ cluster_network_type }} 
Platform: {{ cluster_platform }}

Cluster Stats:
Number of namespaces: {{ cluster_namespaces }}
Number of builds: {{ cluster_builds }}
Number of secrets: {{ cluster_secrets }}
Number of config maps: {{ cluster_configmaps }}
Number of services: {{ cluster_services }}
Number of build configs: {{ cluster_bc }}
Number of custom resource definitions (CRD): {{ cluster_crds }}
----

=== Nodes Information:
==== Nodes: 
----
{{ nodes }} 
----
==== Nodes that are in 'Not Ready' state:
----
{{ nodes_not_ready }}
---- 

=== Pods Information:
==== Pods Not Running
----
{{ pods_not_running | to_nice_yaml }}
----
==== Pods Restarted
----
{{ pods_restarted | to_nice_yaml }}
----

*Observation:*

==== CSR:
----
- {{ oc_csr_pending_fact }}
----

=== *MachineConfig Information*:

The following check gets the names of machine config pools and other relevant information for a consultant's discretion. 
----
{{ machine_config_pools_name }}
----
The following breaks down which nodes are associated into which machine config pool.
----
{{ nodes_mcp }}
----
Degraded machine counts refer to the number of machines in your OCP cluster that are experiencing issues or are in a degraded state. This would affect application availability and resource utilisation. (Preferred State is zero)
----
Degraded MCPs Status: {{ degraded_mcps }}
----
Nodes may be in a pending state that may eventuate to the degraded state, whilst the OCP Health Check is occurring. The prefferred state is each Machine Config Pool having a 0 value. (Read the following as first mcp's unavailable value is the first character of the string and so on)
----
unavailable_nodes_count: "{{ unavailable_nodes_count }}"
----


=== *Resource Quotas*:
----
Name: 
{{ resource_quota_name }}

Hard Limit:
{{ resource_quota_hard_limit }}

Used Limit:
{{ resource_quota_used_limit }}
----
=== *Image Registry*:

The Management State of the Image Registry Operator alters the behaviour of the deployed image pruner job. 
 * 'Managed' means the --prune-registry flage for image pruner is set to true (preferred state).
 * 'Removed' means the --prune-registry flag for the image pruner is set to false, meaning it only prunes image metadata in etcd.
 * 'Unmanaged' means the --prune-registry flag for the image pruner is set to false. 
----
The Management State of Image Registry Operator: {{ management_state_registry }}
----

Builder images are base images that contain the necessary tools and runtime for building and compiling source code into executable applications. Builder images are used as a foundation for creating applicating  images. They are often provided by Openshift, the community, or can be custom-built to suit specific development environments and languages. 

This check is assuming the images are in the openshift-image-registry namespace and/or master nodes. 

The check has found the following images that is not provided by releases of Red Hat and OpenShift. Please review the health of these images through Red Hat Advanced Cluster Security and/or through organisational policies. 
----
{{ external_images_node }}
{{ external_images_registry_namespace }}
----

=== Operators 
==== List of Cluster Service Versions that have not Succeeded:
----
{{ CSV_STATUS }}
----
==== List of install plans that have not been approved: 
----
{{ INSTALL_PLAN }}
----
==== List of operators: 
----
{{ OPERATORS }}
----

=== Networks 
==== List of routes that have not been admitted:
----
{{ routes_not_admitted }}
----

=== Persistent Storage

Persistent storage in OpenShift uses the Kubernetes persistent volume (PV) framework that allows cluster administrators to provision persistent storage for a cluster. Developers use persistent volume claims (PVCs) to request PV resources without having specific knowledge of the underlying storage infrastructure.  PVCs are specific to a project while PV resources on their own are not scoped to any single project. After a PV is bound to a PVC, that PV can not then be bound to additional PVCs.  PVCs can exist in the system that are not owned by any container. This may be intentional, if the PVC is to be retained for future use.

==== Storage Classes

StorageClass objects describes and classifies storage that can be requested and serve as a management mechanism for controlling different levels of storage and access to that storage.  

The following storage classes are defined in the cluster:

{% for sc in storage_classes %}
[cols="1,1"]
|===
|**Name**
|{{ sc.name }}

|**Provisioner**
|{{ sc.provisioner }}

|**Default**
| {{ sc.default }}

|===
{% endfor %}

==== Bound PersistentVolumeClaims

The following list of PersistentVolumeClaims (PVC) are defined and bound to an underlying Persistent Volume (PV) in the cluster across all namespaces:

{% for pvc in bound_pvcs %}

[cols="1,1"]
|===
|**Name **
|{{ pvc.name }}

|**Namespace**
|{{ pvc.namespace }}

|**Storage Class**
|{{ pvc.storageclass }}

|**Capacity**
|{{ pvc.capacity }}

|**Access Modes**
|{{ pvc.accessmodes }}

|===

{% endfor %}

{% if unbound_pvcs|length > 0 %}
==== Un-Bound PVCs

The following list of PersistentVolumeClaims (PVC) are defined and are not bound to any underlying Persistent Volume (PV) in the cluster across all namespaces:

{% for pvc in unbound_pvcs %}

[cols="1,1"]
|===
|**Name **
|{{ pvc.name }}

|**Namespace**
|{{ pvc.namespace }}

|**Storage Class**
|{{ pvc.storageclass }}

|**Capacity**
|{{ pvc.capacity }}

|**Access Modes**
|{{ pvc.accessmodes }}

|===

{% endfor %}

PVCs may be unbound for multiple reasons.  For example: some storage classes will only bind a PVC to a PV when it is actually used.  This may be a desired state.  In other cases, a PVC may not be able to bind to a PV if the Storage Class can not satisfy the storage request if, for example, there is insufficient space available, or if the PVC is requesting an access mode not supported by that Storage Class.

**Consultant Recommendations**

__TODO__
{% endif %}

{% if orphaned_pvcs|length > 0 %}
==== Unowned PVCs

PVCs can exist in the system that are not owned by any container. This may be intentional if, for example, the PVC has been released by an application but it is intended to be reused by another application. Alternatively, the PVC may need to be manually deleted.

Results: 

{% for pvc in orphaned_pvcs %}
- {{ pvc }}

{% endfor %}

**Consultant Recommendations**

__TODO__
{% endif %}

{% if unowned_pvs|length > 0 %}
==== Unowned PVs

Persistent Volumes (PV) can exist in the system that are not owned by any PVC. This may be intentional if, for example, the PV delete policy is set to __Retain__ so that they can be manually deleted after the PVC is deleted.

Results: 

{% for pv in unowned_pvs %}
- {{ pv.name }}

{% endfor %}

**Consultant Recommendations**

__TODO__
{% endif %}

=== *Kubelets*:

==== Authentication
Anonymous authentication should be preferably set to false, in order for users to identify themselves befor authentication to API.
----
{{ anonymous_authentication }}
----
==== Pods
podsPerCore sets the number of pods the node can run based on the number of processor cores on the node.podsPerCore cannot exceed maxPods (default state of maxPods is 250 pods with 4096 podPidsLimit)
----
{{ kubelet_pods}}
----
==== APIs
The rate at which the kubelet talks to the API server depends on queries per second (QPS) and burst values. The default values 50 for kubeAPIQPS and 100 for kubeAPIBurst, are good enough if there are limited pods running on each node. Updating the kubelet QPS and burst rates is recommended if there are enough CPU and memory resources on the node.
----
{{ kubelet_APIs }}
----
==== Rotate Certificates
Having rotate certificates enabled makes sure that nodes are more consistently available, whilst certificates may expire.
----
{{ kubelet_rotate_certificate }}
----
==== CgroupDriver
Cgroupfs and systemd are the predominant cgroup drivers. The preferred driver is systemd as it is tightly integrated with cgroups and will assin a cgroup to each systemd unit. Using cgroupfs with systemd means that there will be two different cgroup managers( ie two views of the resources)
----
{{ kubelet_cgroupDriver }}
----
==== CgroupRoot
CgroupRoot should be roots directory. Ensuring that the kubelet service file ownership is set to root.
----
{{ kubelet_cgroupRoot }}
----
==== Permissions
Ensuring that the kubelet service file permissions are set to 644 or more restrictive.
----
{{ kubelet_permission }}
----
==== clusterDNS
The IP address Pods are using for DNS resolution.
----
{{ kubelet_clusterDNS }}
----